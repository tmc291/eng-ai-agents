# Import necessary PyTorch libraries
import torch
import torch.linalg as linalg
import torch.rand as rand

# 3. Simulation of Gaussian random variables
def simulate_gaussian(mean, std_dev, num_samples):
    return mean + std_dev * torch.randn(num_samples)

# Example
mean = 0.0
std_dev = 1.0
num_samples = 1000
gaussian_samples = simulate_gaussian(mean, std_dev, num_samples)
print("Simulated Gaussian samples:", gaussian_samples)

# 4. KMeans Implementation using PyTorch
class KMeans:
    def __init__(self, num_clusters, max_iters=100):
        self.num_clusters = num_clusters
        self.max_iters = max_iters
    
    def fit(self, data):
        # Initialize centroids randomly from the data
        indices = torch.randperm(len(data))[:self.num_clusters]
        centroids = data[indices]
        
        for _ in range(self.max_iters):
            # Assign each point to the nearest centroid
            distances = linalg.norm(data[:, None] - centroids, dim=2)
            clusters = torch.argmin(distances, dim=1)
            
            # Recompute centroids as the mean of the points in each cluster
            new_centroids = torch.stack([data[clusters == i].mean(dim=0) for i in range(self.num_clusters)])
            
            if torch.allclose(new_centroids, centroids):
                break
            centroids = new_centroids
        
        self.centroids = centroids
        self.clusters = clusters

# Example for KMeans
data = torch.rand(100, 2)
kmeans = KMeans(num_clusters=3)
kmeans.fit(data)
print("KMeans centroids:", kmeans.centroids)
print("Cluster assignments:", kmeans.clusters)

# 5. PCA Implementation using PyTorch
def pca(data, num_components):
    # Center the data
    mean = data.mean(dim=0)
    centered_data = data - mean
    
    # Compute covariance matrix
    cov_matrix = torch.mm(centered_data.T, centered_data) / (data.shape[0] - 1)
    
    # Perform SVD
    U, S, V = linalg.svd(cov_matrix)
    
    # Select the top 'num_components' principal components
    principal_components = V[:num_components]
    
    # Project the data onto the principal components
    reduced_data = torch.mm(centered_data, principal_components.T)
    
    return reduced_data, principal_components

# Example for PCA
data = torch.rand(100, 5)
reduced_data, components = pca(data, num_components=2)
print("Reduced data (first 5 samples):", reduced_data[:5])
print("Principal components:", components)
